{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/solva/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from chimera.model.expert_encoder.configuration_sci_encoder import SciEncoderConfig, init_config_from_meta_config\n",
    "from chimera.model.expert_encoder.modeling_sci_encoder import SciEncoder\n",
    "from transformers import Pix2StructVisionModel, Pix2StructVisionConfig, Pix2StructConfig, Pix2StructForConditionalGeneration, Pix2StructImageProcessor\n",
    "from transformers import CLIPVisionModel, CLIPVisionConfig, CLIPImageProcessor\n",
    "from chimera.model.kosmos2_5.modeling_kosmos2_5 import Kosmos2_5Config, Kosmos2_5ForConditionalGeneration\n",
    "from chimera.model.kosmos2_5 import Kosmos2_5VisionModel, Kosmos2_5VisionConfig, Kosmos2_5ImageProcessor\n",
    "# from chimera.train.internvl_chat_finetune import *\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "internvl_path = '/mnt/workspace/pengtianshuo_hf_ckp/OpenGVLab/InternVL2-8B'\n",
    "p2s_path = '/mnt/workspace/pengtianshuo_hf_ckp/solva_modules/chart_p2s'\n",
    "clip_path = '/mnt/workspace/pengtianshuo_hf_dataset/MAVIS/CLIP-Math/Arxiv-ViT-L-14-336'\n",
    "kosmos_path = '/mnt/workspace/pengtianshuo_hf_ckp/solva_modules/table_kosmos'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kosmos_processor = Kosmos2_5ImageProcessor.from_pretrained(kosmos_path)\n",
    "clip_processor = CLIPImageProcessor.from_pretrained(clip_path)\n",
    "p2s_processor = Pix2StructImageProcessor.from_pretrained(p2s_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CLIPImageProcessor' object has no attribute 'max_patches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclip_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_patches\u001b[49m \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CLIPImageProcessor' object has no attribute 'max_patches'"
     ]
    }
   ],
   "source": [
    "clip_processor.max_patches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kosmos_processor, clip_processor, p2s_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.new('RGB', (224, 224), (255, 255, 255))\n",
    "\n",
    "images = [image, image, image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kosmos2_5ImageProcessor'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kosmos_processor.image_processor_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kosmos_output = kosmos_processor(images, return_tensors=\"pt\")\n",
    "clip_output = clip_processor(images, return_tensors=\"pt\")\n",
    "p2s_output = p2s_processor(images, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kosmos_output.flattened_patches.shape, kosmos_output.attention_mask.shape,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_output.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2s_output.flattened_patches.shape, p2s_output.attention_mask.shape,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm_config = InternVLChatConfig.from_pretrained(internvl_path)\n",
    "vlm_config.vision_config.drop_path_rate = 0.0\n",
    "if vlm_config.llm_config.model_type == 'internlm2':\n",
    "    vlm_config.llm_config.attn_implementation = 'flash_attention_2'  # for InternLM\n",
    "else:\n",
    "    vlm_config.llm_config._attn_implementation = 'flash_attention_2'  # for LLaMA\n",
    "\n",
    "vlm_config.template = \"internlm2-chat\"\n",
    "vlm_config.select_layer = -1\n",
    "vlm_config.dynamic_image_size = True\n",
    "vlm_config.use_thumbnail = True\n",
    "vlm_config.ps_version = 'v2'\n",
    "vlm_config.min_dynamic_patch = 1\n",
    "vlm_config.max_dynamic_patch = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_encoder_meta_config = json.loads(open('/mnt/workspace/Solva/internvl_chat/shell/sci_encoder_config/solva_table_math_chart.json').read())\n",
    "for i in sci_encoder_meta_config:\n",
    "    # sci_encoder_meta_config中每个元素的键\"model_name_or_path\"和\"model_type\"都只在训练中用到，加载模型结束后就会被删除\n",
    "    i[\"config\"] = init_config_from_meta_config(i).to_dict()\n",
    "    name_or_path = i.pop(\"model_name_or_path\")\n",
    "    i[\"config\"]['_name_or_path'] = name_or_path\n",
    "    \n",
    "sci_config = SciEncoderConfig(sci_encoder_meta_config, vlm_config.llm_config.hidden_size)\n",
    "# 分别加载来自不同路径的encoder\n",
    "sci_config.separate_load = True\n",
    "vlm_config.sci_encoder_config = sci_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InternVLChatModel.from_pretrained(\n",
    "    internvl_path, torch_dtype=torch.bfloat16, config=vlm_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    internvl_path, add_eos_token=False, trust_remote_code=True, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_token_list = []\n",
    "token_list = [IMG_START_TOKEN, IMG_END_TOKEN, IMG_CONTEXT_TOKEN,\n",
    "                  QUAD_START_TOKEN, QUAD_END_TOKEN, REF_START_TOKEN,\n",
    "                  REF_END_TOKEN, BOX_START_TOKEN, BOX_END_TOKEN]\n",
    "for i in range(3):\n",
    "    sci_token_list.append(f\"<DOMAIN_{i}_CONTEXT>\")\n",
    "token_list.extend(sci_token_list)\n",
    "\n",
    "num_new_tokens = tokenizer.add_tokens(token_list, special_tokens=True)\n",
    "img_context_token_id = tokenizer.convert_tokens_to_ids(IMG_CONTEXT_TOKEN)\n",
    "model.img_context_token_id = img_context_token_id\n",
    "\n",
    "\n",
    "sci_context_token_id = tokenizer.convert_tokens_to_ids(sci_token_list)\n",
    "model.set_domain_context_token_ids(sci_context_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
